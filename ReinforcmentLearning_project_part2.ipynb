{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.11191734 0.79896733 ... 0.68292715 0.03872294 0.18091375]\n",
      " [0.52668159 0.         0.8884091  ... 0.71369419 0.39868969 0.52831082]\n",
      " [0.92881528 0.24741652 0.         ... 0.61819665 0.34488573 0.46754078]\n",
      " ...\n",
      " [0.55356429 0.38491872 0.34322512 ... 0.         0.04005022 0.56609825]\n",
      " [0.44808562 0.24297506 0.27416776 ... 0.24672608 0.         0.05742574]\n",
      " [0.91890314 0.99584594 0.83795267 ... 0.97545181 0.14907142 0.        ]]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import minimize, Bounds, linprog\n",
    "# Environment parameters\n",
    "K = 100  # Number of content items\n",
    "u_min = 0.8  # Threshold for content relevance\n",
    "C = int(0.8 * K)  # Number of cached items\n",
    "#C = 2\n",
    "# User model parameters\n",
    "N = 4 # Number of recommended items\n",
    "q = 0.2  # Probability of ending the viewing session\n",
    "alpha = 0.8  # Probability of selecting a recommended item\n",
    "#tradeoff_factor=0.4\n",
    "# Generate random relevance values\n",
    "U = np.random.rand(K, K)\n",
    "np.fill_diagonal(U, 0)  # Set diagonal elements to 0\n",
    "# U = np.array([[0., 0.09709217, 0.95697935, 0.76421269, 0.79379138],\n",
    "#               [0.85679266, 0., 0.73115609, 0.97025111, 0.00706508],\n",
    "#               [0.38327773, 0.27582305, 0., 0.40938946, 0.70918518],\n",
    "#               [0.27415892, 0.89691232, 0.47103534, 0., 0.97776446],\n",
    "#               [0.06699551, 0.96500574, 0.00547615, 0.74654658, 0.]])\n",
    "# U = np.array([[0.0, 0.8, 0.3, 0.6],\n",
    "#               [0.8, 0.0, 0.7, 0.2],\n",
    "#               [0.3, 0.1, 0.0, 0.2],\n",
    "#               [0.6, 0.4, 0.2, 0.0]])\n",
    "print(U)\n",
    "#vector to denote the cost of each state. 1 for non-cached, 0 for cached\n",
    "Cost = [1]*(K-C) +[0]*C   \n",
    "#Cost = [1,0,1,0]\n",
    "random.shuffle(Cost)\n",
    "print(Cost)\n",
    "\n",
    "Tmax = 10000\n",
    "\n",
    "#create action set as the set of every possible combination of N=2 states \n",
    "# action_set = []\n",
    "# for i in range(K):\n",
    "#     for j in range(i+1,K):\n",
    "#         a = (i, j)\n",
    "#         action_set.append(a)\n",
    "\n",
    "# num_of_actions = len(action_set)\n",
    "\n",
    "# action_table = [[] for _ in range(K)]\n",
    "\n",
    "# for i in range(K):\n",
    "#     for a in action_set:\n",
    "#         if i not in a:\n",
    "#             action_table[i].append(a)\n",
    "# num_of_actions = len(action_table[0])\n",
    "# print(num_of_actions)\n",
    "# print (action_table)\n",
    "# a = random.choice(action_table[2])\n",
    "# s_prime = int(np.random.choice(a))\n",
    "# print(a)\n",
    "# print(s_prime)\n",
    "\n",
    "def normalize_matrix_R(R):\n",
    "    row_sums = R.sum(axis=1)\n",
    "\n",
    "    # New matrix is old matrix divided by row sums\n",
    "    # Use np.newaxis to match the dimensions for broadcasting\n",
    "    R = N*R / row_sums[:, np.newaxis]\n",
    "    return R\n",
    "\n",
    "# R = normalize_matrix_R(U)\n",
    "# print(R.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Policy Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def all_recommendations_are_relevant(recommendations,s):\n",
    "    \"\"\"\n",
    "    function to check whether everey recommended state in the racommendation batch is \n",
    "    relevant to s\n",
    "\n",
    "    arguments:\n",
    "    recommendations (tuple of ints): recommendation batch for state s\n",
    "    s (int): current state\n",
    "\n",
    "    returns:\n",
    "    True: if all recommendation are relevant to s\n",
    "    False: otherwise\n",
    "    \"\"\"\n",
    "    for u in recommendations:\n",
    "        if U[s][int(u)]<u_min:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# def get_next_states(s, r): \n",
    "#     \"\"\"\n",
    "#     function to generate all possible next states alongiside their respective probabilities and rewards\n",
    "#     given a state s and an action a\n",
    "\n",
    "#     arguments:\n",
    "#     s (int):  current state\n",
    "#     a (tuple of ints): recommendation batch for state s\n",
    "\n",
    "#     returns:\n",
    "#     next_states (list of tuples):a list of tuples, where each tuple denotes the next state alongiside its respective probability and reward\n",
    "#     \"\"\"\n",
    "#     next_states = []\n",
    "    \n",
    "#     for s_prime in range(K):\n",
    "        \n",
    "#         prob = (1-N*u_min)*(alpha * r[s_prime]/N + (1-alpha)/K) +N/K*u_min #probability to choose a video from the recommedation batch \n",
    "#         reward = 1/2-Cost[s_prime] #define reward to be 1 for cached and 0 for uncached video\n",
    "#         next_states.append((prob,s_prime,reward))\n",
    "        \n",
    "                \n",
    "#     return next_states\n",
    "       \n",
    "# def policy_evaluation(R, gamma = 1.0, epsilon = 1e-10):  \n",
    "#     \"\"\"\n",
    "#     Function to evaluate a given policy\n",
    "\n",
    "#     arguments:\n",
    "#     pi (list of tuples): policy to be evaluated\n",
    "#     gamma (float): discounting factor\n",
    "#     epsilon (float): approximation tolerance\n",
    "\n",
    "#     returns:\n",
    "#     V (vector of floats): vector of value function of each state calculated using bellman equation\n",
    "#     \"\"\"\n",
    "\n",
    "#     prev_V = np.zeros(K) # use as \"cost-to-go\", i.e. for V(s')\n",
    "#     while True: #performing iterations\n",
    "#         V = np.zeros(K) # current value function to be learnerd\n",
    "#         for s in range(K):  # do for every state\n",
    "#             for prob, next_state, reward in get_next_states(s, R[s]):  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "#                 V[s] += prob * (reward + gamma * prev_V[next_state] )\n",
    "#         if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one; \n",
    "#             break # if yes, finish loop\n",
    "#         prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "#     return V\n",
    "\n",
    "\n",
    "\n",
    "# # Define the function to minimize\n",
    "# def func(x,V):\n",
    "#         return -np.dot(x , V )\n",
    "\n",
    "# def policy_improvement(R, V, gamma=1.0):  \n",
    "#     \"\"\"\n",
    "#     Function to greedily improve policy\n",
    "\n",
    "#     arguments:\n",
    "#     V (vector of floats): V (vector of floats): vector of value function of each state calculated using bellman equation\n",
    "#     gamma (float): discounting factor\n",
    "    \n",
    "#     returns:\n",
    "#     new_pi (list of tuples): new policy\n",
    "#     \"\"\"\n",
    "   \n",
    "#     # Define the equality constraint\n",
    "    \n",
    "#     new_R = np.zeros((K, K), dtype=np.float64) #create a Q value array\n",
    "#     # # Print the result\n",
    "#     # print(f\"Minimized function value: {result.fun}\")\n",
    "#     # print(f\"Argmin: {result.x}\")\n",
    "\n",
    "#     for s in range(K):\n",
    "        \n",
    "#         constraints = [{'type': 'eq', 'fun': lambda x: sum(x) - N},  # Sum of elements should be N\n",
    "#                         {'type': 'eq', 'fun': lambda x: x[s]}]\n",
    "#         # Define the bounds (0 <= v(i) <= 1 for all i)\n",
    "#         bounds = Bounds([0]*K, [1]*K)\n",
    "\n",
    "#         # Initial guess\n",
    "#         x0 = R[s]  # Initial guess should respect the constraints\n",
    "\n",
    "#         # Call the minimize() function with the 'SLSQP' method, bounds and constraints\n",
    "#         result = minimize(func, args = (V), x0=x0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "#         new_R[s] = result.x\n",
    "#     return new_R\n",
    "\n",
    "\n",
    "# def policy_iteration( gamma = 1.0, epsilon = 1e-10):\n",
    "#     \"\"\"\n",
    "#     Function to converge to the optimal policy by performing iterative avaluation and improvments until convergence\n",
    "\n",
    "#     arguments:\n",
    "#     gamma (float): discounting factor\n",
    "#     epsilon (float): approximation tolerance\n",
    "    \n",
    "#     returns:\n",
    "#     pi (list of tuples): optimal policy\n",
    "#     V (vector of floats): vector of value function of each state calculated using bellman equation\n",
    "#     \"\"\"\n",
    "#     t = 0\n",
    "#     # random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state  \n",
    "#     # pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
    "#     R = normalize_matrix_R(U)\n",
    "#     pi = np.random.randint(0, K, size=(K, N))  # Random initial policy\n",
    "#     #print(np.sum(R,axis=1))\n",
    "\n",
    "#     while True:\n",
    "#         prev_R = R.copy()  #keep the old policy to compare with new\n",
    "#         V = policy_evaluation(R,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "\n",
    "#         R = policy_improvement(R,V,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "#         t += 1\n",
    "\n",
    "#         if (np.max(np.abs(prev_R - R)) < epsilon): # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "#             break\n",
    "#     print('converged after %d iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    \n",
    "#     #Construct a new police by selecting N items withs max r_ij\n",
    "#     pi = np.argpartition(R, -N, axis=1)[:, -N:]\n",
    "\n",
    "#     # Construct the KxN matrix using the indices\n",
    "    \n",
    "#     return V,pi\n",
    "    \n",
    "# def compare_two_policies(pi,old_pi):\n",
    "#     \"\"\"\n",
    "#     Function to compare if two policies are identical\n",
    "\n",
    "#     arguments:\n",
    "#     pi: current policy\n",
    "#     old: previous policy\n",
    "\n",
    "#     returns:\n",
    "#     True if they are identical\n",
    "#     Flase otherwise\n",
    "#     \"\"\"\n",
    "#     for k in range(K):\n",
    "#         for n in range(N):\n",
    "#             if (pi[k][n]!=old_pi[k][n]):\n",
    "#                 return False\n",
    "\n",
    "#     return True\n",
    "\n",
    "\n",
    "# V_opt,P_opt1 = policy_iteration(1-q,0.001)   #just example of calling the various new functions we created.\n",
    "\n",
    "\n",
    "\n",
    "# print(\"#############################\")\n",
    "# #print(V_opt)\n",
    "# print(\"#############################\")\n",
    "# print(P_opt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.11191734 0.79896733 ... 0.68292715 0.03872294 0.18091375]\n",
      " [0.52668159 0.         0.8884091  ... 0.71369419 0.39868969 0.52831082]\n",
      " [0.92881528 0.24741652 0.         ... 0.61819665 0.34488573 0.46754078]\n",
      " ...\n",
      " [0.55356429 0.38491872 0.34322512 ... 0.         0.04005022 0.56609825]\n",
      " [0.44808562 0.24297506 0.27416776 ... 0.24672608 0.         0.05742574]\n",
      " [0.91890314 0.99584594 0.83795267 ... 0.97545181 0.14907142 0.        ]]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[[ 0.05052386  0.02097428  0.03126905 ... -0.0744434   0.070451\n",
      "   0.03091439]\n",
      " [ 0.02059317  0.05054273  0.02015761 ... -0.07358232  0.0614256\n",
      "   0.        ]\n",
      " [ 0.04048993  0.04109222  0.05083074 ... -0.04677171  0.08036604\n",
      "   0.08032072]\n",
      " ...\n",
      " [ 0.05135885  0.03048974  0.0411244  ... -0.03873579  0.03061229\n",
      "   0.06010694]\n",
      " [ 0.08059833  0.0609217   0.08039426 ... -0.01932571  0.06127665\n",
      "   0.03063946]\n",
      " [ 0.11852236  0.13817019  0.04038892 ... -0.04814276  0.04129335\n",
      "   0.02083378]]\n",
      "===================================================================================\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n",
      "[[90 22 69 72]\n",
      " [71  8 89 26]\n",
      " [26 35 85 87]\n",
      " [60 88 21 86]\n",
      " [96 63 68  1]\n",
      " [47  7 77 26]\n",
      " [82 34 43 86]\n",
      " [53 61 32 75]\n",
      " [51 40 39  0]\n",
      " [ 2 16 46 57]\n",
      " [57 43 88 47]\n",
      " [ 2 72 20 31]\n",
      " [30 64 22 18]\n",
      " [11 46  6 14]\n",
      " [72 86 60 26]\n",
      " [65 56 35 12]\n",
      " [56 67 90  1]\n",
      " [44 43 11 68]\n",
      " [45  9 55 85]\n",
      " [32 42 59 79]\n",
      " [56 70 59 99]\n",
      " [39 55 16 73]\n",
      " [82 16 51 56]\n",
      " [52 12 30 20]\n",
      " [61 45 95 16]\n",
      " [76 87 29 73]\n",
      " [21 39 59 68]\n",
      " [98 32 19 71]\n",
      " [21 95 38 63]\n",
      " [11 20 16 26]\n",
      " [34 85 70 56]\n",
      " [14 55 64 26]\n",
      " [47 11 72 51]\n",
      " [57 56 38 99]\n",
      " [22 90 83 12]\n",
      " [23 65 72 38]\n",
      " [71 57 64 37]\n",
      " [ 1  8 53 99]\n",
      " [45 12 46 42]\n",
      " [ 8 29 69  0]\n",
      " [ 4 58 30 26]\n",
      " [77 55 66 78]\n",
      " [29 19 59 38]\n",
      " [70 63 30 67]\n",
      " [76  9 39  1]\n",
      " [47 34 65  1]\n",
      " [57 32  8  0]\n",
      " [78 32 90  1]\n",
      " [36 67 21 50]\n",
      " [32  4 90 61]\n",
      " [62  8 79 49]\n",
      " [78 19 16 86]\n",
      " [61 21 95 31]\n",
      " [24 82 69 49]\n",
      " [18 21 58  4]\n",
      " [88 45 59 35]\n",
      " [72 43 46 87]\n",
      " [67 22 63  1]\n",
      " [75 11 72 49]\n",
      " [50 32 58 26]\n",
      " [98 89  8 36]\n",
      " [18 22 21  0]\n",
      " [86 88 72 99]\n",
      " [29 66 69 76]\n",
      " [47 19 79  0]\n",
      " [60 40 39 30]\n",
      " [ 8 22 20  1]\n",
      " [ 2  5 45 65]\n",
      " [76 85 26 47]\n",
      " [40 65 38 18]\n",
      " [ 7 30 90 50]\n",
      " [76  6  2 36]\n",
      " [34 20  5 67]\n",
      " [38 40 98 87]\n",
      " [86 55 34 49]\n",
      " [30  8 20 99]\n",
      " [98 71 87  7]\n",
      " [22 78 46 42]\n",
      " [67  6 69 40]\n",
      " [70 43 52 99]\n",
      " [72 31 29 73]\n",
      " [51 14 72 49]\n",
      " [71 63  8 69]\n",
      " [ 7 19 35 73]\n",
      " [72 39  2 76]\n",
      " [72 67 40 47]\n",
      " [23 12 96  7]\n",
      " [40 55 95 99]\n",
      " [ 4 29 31 56]\n",
      " [52 30 11 14]\n",
      " [11 79 96 76]\n",
      " [45 40 79 23]\n",
      " [31 53 21 76]\n",
      " [77 42 76 23]\n",
      " [64 30 29 12]\n",
      " [22 42 39  0]\n",
      " [86 53 61 62]\n",
      " [76 72 12 19]\n",
      " [76 70 16 31]\n",
      " [ 1 82 66  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def func(x,Q):\n",
    "        return -np.dot(x , Q )/N\n",
    "\n",
    "def maximize_model(Q,s):\n",
    "    new_P = np.zeros((K, K), dtype=np.float64) #create a Q value array\n",
    "    # # Print the result\n",
    "    # print(f\"Minimized function value: {result.fun}\")\n",
    "    # print(f\"Argmin: {result.x}\")\n",
    "\n",
    "    \n",
    "    C=-1/N*Q\n",
    "    # Equality constraint for summation: sum(x) = N\n",
    "    A_eq = [[1] * K]\n",
    "    b_eq = [N]\n",
    "\n",
    "    # Adding the specific constraint x_i = 0\n",
    "    # You can directly specify this in bounds, no need to add in A_eq\n",
    "\n",
    "    # Bounds for each variable: 0 <= x_j <= 1 and x_i = 0\n",
    "    bounds = [(0, 1) if j != s else (0, 0) for j in range(K)]\n",
    "\n",
    "    # Now use linprog\n",
    "    result = linprog(C, bounds=bounds, A_eq=A_eq, b_eq=b_eq, method='highs')\n",
    "    new_P = result.x\n",
    "    max = -result.fun\n",
    "    return new_P,max\n",
    "\n",
    "def maximize_model2(P,Q,s):\n",
    "    new_P = np.zeros((K, K), dtype=np.float64) #create a Q value array\n",
    "    # # Print the result\n",
    "    # print(f\"Minimized function value: {result.fun}\")\n",
    "    # print(f\"Argmin: {result.x}\")\n",
    "\n",
    "    \n",
    "        \n",
    "    constraints = [{'type': 'eq', 'fun': lambda x: sum(x) - N},  # Sum of elements should be N\n",
    "                    {'type': 'eq', 'fun': lambda x: x[s]}]\n",
    "    # Define the bounds (0 <= v(i) <= 1 for all i)\n",
    "    bounds = Bounds([0]*K, [1]*K)\n",
    "\n",
    "    # Initial guess\n",
    "    x0 = P  # Initial guess should respect the constraints\n",
    "\n",
    "    # Call the minimize() function with the 'SLSQP' method, bounds and constraints\n",
    "    result = minimize(func, args = (Q), x0=x0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    new_P = result.x\n",
    "    max = -result.fun\n",
    "    return new_P,max\n",
    "\n",
    "\n",
    "def Q_learning(gamma, epsilon,learning_rate):\n",
    "    \"\"\"\n",
    "    Function to perform Q learning algorithm.\n",
    "\n",
    "    arguments:\n",
    "    gamma (float): discounting factor\n",
    "    epsilon (float): epslilon greedy probability\n",
    "    learning_rate (float): learning rate\n",
    "\n",
    "    returns:\n",
    "    Q (matrix K x num_of_actions):  martix of state action value function calculated using bellman equation\n",
    "    \"\"\"\n",
    "    Q = np.zeros((K,K)) \n",
    "    P = normalize_matrix_R(U) \n",
    "    #prev_Q = np.zeros((K,K))\n",
    "    t = 0\n",
    "    while True:\n",
    "        s = np.random.randint(K) #random initial state\n",
    "        while True:\n",
    "            if np.random.uniform() < epsilon:  # Explore if e(t) times\n",
    "                \n",
    "                numbers = list(range(K))\n",
    "                numbers.remove(s)\n",
    "\n",
    "                # Sample two distinct integers from the list\n",
    "                a = tuple(random.sample(numbers, N))\n",
    "                #s_prime = np.random.randint(K) #choose random action\n",
    "                    \n",
    "            else:  # Exploit 1-e(t) times\n",
    "                a= tuple(np.argpartition(-np.array(P[s]), N)[:N])\n",
    "                #s_prime = np.argmax(Q[s]) #choose greedily the action with highest Q value\n",
    "            # a = action_table[s][a_idx]  \n",
    "\n",
    "\n",
    "            if (all_recommendations_are_relevant(a,s)):\n",
    "            \n",
    "                \n",
    "                s_prime = int(np.random.choice(a))  # Pick a random item from relevant recommended items\n",
    "                \n",
    "            else:\n",
    "                s_prime = np.random.randint(K)  # Pick a random item\n",
    "\n",
    "            if np.random.uniform() < q: #if user opt to terminate session\n",
    "                target = (1 - 2*Cost[s_prime])\n",
    "                Q[s][s_prime] = Q[s][s_prime] + learning_rate * ( target - Q[s][s_prime] )\n",
    "                break\n",
    "            else:\n",
    "                P[s_prime],max = maximize_model(Q[s_prime],s_prime)\n",
    "                target = (1 - 2*Cost[s_prime]) + gamma*max\n",
    "            Q[s][s_prime] = Q[s][s_prime] + learning_rate * ( target - Q[s][s_prime] )\n",
    "            \n",
    "            s = s_prime\n",
    "        t+=1\n",
    "        epsilon = (t+1)**(-1/3)*(K*math.log(t+1))**(1/3)\n",
    "        #epsilon = 0.1\n",
    "        #learning_rate = learning_rate*(1/t)**(1/2)\n",
    "        \n",
    "        #if (np.max(np.abs(prev_Q - Q)) < delta and t>1000*K) or \n",
    "        if t > 100*K: #check if the new V estimate is close enough to the previous one;\n",
    "            break # if yes, finish loop\n",
    "        #prev_Q = Q.copy()\n",
    "    print(Q)\n",
    "    print(\"===================================================================================\")\n",
    "    print(P)\n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "# pi_Q_learning  =  np.zeros((K, N), dtype=np.int16)\n",
    "print(U)\n",
    "print(Cost)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "Q = Q_learning(1-q,1,0.01)\n",
    "\n",
    "pi_Q_learning = np.argpartition(Q, -N, axis=1)[:, -N:]\n",
    "# for s in range(K):\n",
    "#     #Q[s][s] = float('-inf')\n",
    "#     action = np.argmax(Q[s])\n",
    "#     pi_Q_learning[s] = action_table[s][action]\n",
    "\n",
    "print(pi_Q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.11191734 0.79896733 ... 0.68292715 0.03872294 0.18091375]\n",
      " [0.52668159 0.         0.8884091  ... 0.71369419 0.39868969 0.52831082]\n",
      " [0.92881528 0.24741652 0.         ... 0.61819665 0.34488573 0.46754078]\n",
      " ...\n",
      " [0.55356429 0.38491872 0.34322512 ... 0.         0.04005022 0.56609825]\n",
      " [0.44808562 0.24297506 0.27416776 ... 0.24672608 0.         0.05742574]\n",
      " [0.91890314 0.99584594 0.83795267 ... 0.97545181 0.14907142 0.        ]]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "[[90 22 69 72]\n",
      " [71  8 89 26]\n",
      " [26 35 85 87]\n",
      " [60 88 21 86]\n",
      " [96 63 68  1]\n",
      " [47  7 77 26]\n",
      " [82 34 43 86]\n",
      " [53 61 32 75]\n",
      " [51 40 39  0]\n",
      " [ 2 16 46 57]\n",
      " [57 43 88 47]\n",
      " [ 2 72 20 31]\n",
      " [30 64 22 18]\n",
      " [11 46  6 14]\n",
      " [72 86 60 26]\n",
      " [65 56 35 12]\n",
      " [56 67 90  1]\n",
      " [44 43 11 68]\n",
      " [45  9 55 85]\n",
      " [32 42 59 79]\n",
      " [56 70 59 99]\n",
      " [39 55 16 73]\n",
      " [82 16 51 56]\n",
      " [52 12 30 20]\n",
      " [61 45 95 16]\n",
      " [76 87 29 73]\n",
      " [21 39 59 68]\n",
      " [98 32 19 71]\n",
      " [21 95 38 63]\n",
      " [11 20 16 26]\n",
      " [34 85 70 56]\n",
      " [14 55 64 26]\n",
      " [47 11 72 51]\n",
      " [57 56 38 99]\n",
      " [22 90 83 12]\n",
      " [23 65 72 38]\n",
      " [71 57 64 37]\n",
      " [ 1  8 53 99]\n",
      " [45 12 46 42]\n",
      " [ 8 29 69  0]\n",
      " [ 4 58 30 26]\n",
      " [77 55 66 78]\n",
      " [29 19 59 38]\n",
      " [70 63 30 67]\n",
      " [76  9 39  1]\n",
      " [47 34 65  1]\n",
      " [57 32  8  0]\n",
      " [78 32 90  1]\n",
      " [36 67 21 50]\n",
      " [32  4 90 61]\n",
      " [62  8 79 49]\n",
      " [78 19 16 86]\n",
      " [61 21 95 31]\n",
      " [24 82 69 49]\n",
      " [18 21 58  4]\n",
      " [88 45 59 35]\n",
      " [72 43 46 87]\n",
      " [67 22 63  1]\n",
      " [75 11 72 49]\n",
      " [50 32 58 26]\n",
      " [98 89  8 36]\n",
      " [18 22 21  0]\n",
      " [86 88 72 99]\n",
      " [29 66 69 76]\n",
      " [47 19 79  0]\n",
      " [60 40 39 30]\n",
      " [ 8 22 20  1]\n",
      " [ 2  5 45 65]\n",
      " [76 85 26 47]\n",
      " [40 65 38 18]\n",
      " [ 7 30 90 50]\n",
      " [76  6  2 36]\n",
      " [34 20  5 67]\n",
      " [38 40 98 87]\n",
      " [86 55 34 49]\n",
      " [30  8 20 99]\n",
      " [98 71 87  7]\n",
      " [22 78 46 42]\n",
      " [67  6 69 40]\n",
      " [70 43 52 99]\n",
      " [72 31 29 73]\n",
      " [51 14 72 49]\n",
      " [71 63  8 69]\n",
      " [ 7 19 35 73]\n",
      " [72 39  2 76]\n",
      " [72 67 40 47]\n",
      " [23 12 96  7]\n",
      " [40 55 95 99]\n",
      " [ 4 29 31 56]\n",
      " [52 30 11 14]\n",
      " [11 79 96 76]\n",
      " [45 40 79 23]\n",
      " [31 53 21 76]\n",
      " [77 42 76 23]\n",
      " [64 30 29 12]\n",
      " [22 42 39  0]\n",
      " [86 53 61 62]\n",
      " [76 72 12 19]\n",
      " [76 70 16 31]\n",
      " [ 1 82 66  0]]\n",
      "average cost for Policy iteration:\n",
      "average cost for Q Learning:\n",
      "0.98446\n"
     ]
    }
   ],
   "source": [
    "def simulate_session(policy, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Simulate a viewing session following a given policy\n",
    "\n",
    "    arguments:\n",
    "    policy to be simulated\n",
    "\n",
    "    returns:\n",
    "    total cost of the session\n",
    "    \n",
    "    \"\"\"\n",
    "    s = np.random.randint(K)  # random initial\n",
    "    cost_total = Cost[s]  \n",
    "    for _ in range(max_steps):\n",
    "        if np.random.uniform() < q:  # The user decides to quit\n",
    "            break\n",
    "\n",
    "        if (all_recommendations_are_relevant(policy[s],s)):\n",
    "            \n",
    "            if np.random.uniform() < alpha:  # If all recommended items are relevant\n",
    "                s_prime = int(np.random.choice(policy[s]))  # Pick a random item from relevant recommended items\n",
    "            else:  # If at least one recommended item is not relevant\n",
    "                s_prime = np.random.randint(K)  # Pick a random item\n",
    "        else:\n",
    "            s_prime = np.random.randint(K)  # Pick a random item\n",
    "        \n",
    "        s=s_prime\n",
    "        cost_total += Cost[s]  # Add the cost of the picked item\n",
    "    return cost_total\n",
    "\n",
    "def simulation(policy):\n",
    "    \"\"\"\n",
    "    function to run multiple sessions\n",
    "    \"\"\"\n",
    "    total_cost = 0\n",
    "    num_of_episodes=50000\n",
    "    for _ in range(num_of_episodes):\n",
    "        total_cost  += simulate_session(policy)\n",
    "\n",
    "    print(total_cost/num_of_episodes)\n",
    "\n",
    "print(U)\n",
    "print(Cost)\n",
    "#print(P_opt1)\n",
    "print(pi_Q_learning)\n",
    "\n",
    "print(\"average cost for Policy iteration:\")\n",
    "#simulation(P_opt1)\n",
    "#simulation(P_opt2)\n",
    "print(\"average cost for Q Learning:\")\n",
    "simulation(pi_Q_learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
